{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2次元の降下法最適化を提供する。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'2次元の降下法最適化を提供する。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescentMethod2D(object):\n",
    "    '''\n",
    "    最適化を提供する。\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.name='DescentMethod2D'\n",
    "    def execute(self,f,w_0:np.ndarray,step:int):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : object\n",
    "            2変数関数を表すオブジェクト。関数の傾きが\n",
    "            f.grad(w:np.ndarray)->np.ndarray\n",
    "            によって得られる必要がある。\n",
    "        w_0 : np.ndarray\n",
    "            [x_0,y_0]と入れること。\n",
    "        Returns\n",
    "        ----x---\n",
    "        w : np.ndarray\n",
    "            最終的な[x,y]\n",
    "        x_log : list\n",
    "            全てのステップにおけるxの履歴\n",
    "        y_log : list\n",
    "            全てのステップにおけるyの履歴\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent(DescentMethod2D):\n",
    "    '最も基本的な最急降下法。'\n",
    "    def __init__(self,eta=0.01):\n",
    "        '''\n",
    "        eta : float\n",
    "            学習率。\n",
    "        '''\n",
    "        self.eta=eta\n",
    "        self.name='SGD η={}'.format(self.eta)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        for i in range(step):\n",
    "            g=f.grad(w) #今の場所の傾き\n",
    "            w-=g*self.eta #傾きの逆方向へ移動\n",
    "            \n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD(DescentMethod2D):\n",
    "    '慣性を考慮した降下法。'\n",
    "    def __init__(self,eta=0.01,gamma=0.9):\n",
    "        '''\n",
    "        eta : float\n",
    "            学習率。\n",
    "        gamma : float\n",
    "            前回の慣性を記憶する割合。\n",
    "        '''\n",
    "        self.eta=eta\n",
    "        self.gamma=gamma\n",
    "        self.name='MomentumSGD η={} γ={}'.format(self.eta,self.gamma)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        m=np.array([0,0])#慣性\n",
    "        for i in range(step):\n",
    "            g=f.grad(w) #今の場所の傾き\n",
    "            m=m*self.gamma + g*self.eta #前回の慣性と傾きから今の慣性を求める\n",
    "            w-=m #慣性に従って移動\n",
    "            \n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovAcceleratedGradient(DescentMethod2D):\n",
    "    '慣性を考える際に一歩先を見る降下法。'\n",
    "    def __init__(self,eta=0.01,gamma=0.9):\n",
    "        '''\n",
    "        eta : float\n",
    "            学習率。\n",
    "        gamma : float\n",
    "            前回の慣性を記憶する割合。\n",
    "        '''\n",
    "        self.eta=eta\n",
    "        self.gamma=gamma\n",
    "        self.name='NAG η={} γ={}'.format(self.eta,self.gamma)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        m=np.array([0,0])#慣性\n",
    "        for i in range(step):\n",
    "            g=f.grad(w-m) #ちょっと先の場所の傾き\n",
    "            m=m*self.gamma + g*self.eta #前回の慣性と傾きから今の慣性を求める\n",
    "            w-=m #慣性に従って移動\n",
    "            \n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad(DescentMethod2D):\n",
    "    '変化具合に応じて学習率を変える降下法。'\n",
    "    def __init__(self,eta0=0.001,epsilon=1e-8):\n",
    "        '''\n",
    "        eta0 : float\n",
    "            学習率の初期値。\n",
    "        epsilon : float\n",
    "            ゼロ除算を防止するための小さな値。\n",
    "        '''\n",
    "        self.eta0=eta0\n",
    "        self.epsilon=epsilon\n",
    "        self.name='AdaGrad η0={} ε={}'.format(eta0,epsilon)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        v=np.repeat(.0,2) #vの初期値\n",
    "        for i in range(step):\n",
    "            g=f.grad(w) #今の場所の傾き\n",
    "            v+=g*g #vの更新\n",
    "            w-=self.eta0/(np.sqrt(v)+self.epsilon)*g #移動 (eta/sqrt(v)+epsilon)*g\n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop(DescentMethod2D):\n",
    "    '最近の変化具合を記憶しながら学習率を調整する降下法。'\n",
    "    def __init__(self,eta0=0.01,gamma=0.99,epsilon=1e-8):\n",
    "        '''\n",
    "        eta0 : float\n",
    "            学習率の初期値。\n",
    "        gamma : float\n",
    "            前回のvを記憶する割合。\n",
    "        epsilon : float\n",
    "            ゼロ除算を防止するための小さな値。\n",
    "        '''\n",
    "        self.eta0=eta0\n",
    "        self.gamma=gamma\n",
    "        self.one_minus_gamma=1-gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.name='RMSprop η0={} γ={} ε={}'.format(eta0,gamma,epsilon)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        v=np.repeat(0,2) #vの初期値\n",
    "        for i in range(step):\n",
    "            g=f.grad(w) #今の場所の傾き\n",
    "            v=self.gamma*v + self.one_minus_gamma*g*g #vの更新\n",
    "            w-=self.eta0/(np.sqrt(v)+self.epsilon)*g #移動\n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaDelta(DescentMethod2D):\n",
    "    '学習率の単位を揃えた降下法。'\n",
    "    def __init__(self,gamma=0.95,epsilon=1e-6):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "            前回の各パラメータを記憶する割合。\n",
    "        epsilon : float\n",
    "            ゼロ除算を防止するための小さな値。\n",
    "        '''\n",
    "        self.gamma=gamma\n",
    "        self.one_minus_gamma=1.-gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.name='AdaDelta γ={} ε={}'.format(gamma,epsilon)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        delta_w=np.repeat(.0,2)\n",
    "        s=np.repeat(.0,2)\n",
    "        v=np.repeat(.0,2)\n",
    "        for i in range(step):\n",
    "            g=f.grad(w) #今の場所の傾き\n",
    "            v=self.gamma*v + self.one_minus_gamma*g*g\n",
    "            s=self.gamma*s + self.one_minus_gamma*delta_w*delta_w\n",
    "            delta_w= -g* np.sqrt( (s+self.epsilon)/(v+self.epsilon) )\n",
    "            w+=delta_w\n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(DescentMethod2D):\n",
    "    'みんな大好きアダムオプティマイザ。'\n",
    "    def __init__(self,eta=0.001,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            学習率。\n",
    "        beta1 : float\n",
    "            前回のmを引き継ぐ割合。\n",
    "        beta2 : float\n",
    "            前回のvを引き継ぐ割合。\n",
    "        epsilon : float\n",
    "            ゼロ除算を防止するための小さな値。\n",
    "        '''\n",
    "        self.eta=eta\n",
    "        self.beta1=beta1\n",
    "        self.one_minus_beta1=1-beta1\n",
    "        self.beta2=beta2\n",
    "        self.one_minus_beta2=1-beta2\n",
    "        self.epsilon=epsilon\n",
    "        self.name='Adam η={} β1={} β2={} ε={}'.format(eta,beta1,beta2,epsilon)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        m=np.repeat(.0,2)\n",
    "        v=np.repeat(.0,2)\n",
    "        for i in range(step):\n",
    "            g=f.grad(w)\n",
    "            m= self.beta1*m + self.one_minus_beta1*g\n",
    "            v= self.beta2*v + self.one_minus_beta2*g*g\n",
    "            estimated_m=m/self.one_minus_beta1\n",
    "            estimated_v=v/self.one_minus_beta2\n",
    "            w=w-self.eta/(np.sqrt(estimated_v)+self.epsilon)*estimated_m\n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDProp(DescentMethod2D):\n",
    "    'NTTの考案したもの。'\n",
    "    def __init__(self,eta=0.001,gamma=0.99,epsilon=1e-8):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            学習率。\n",
    "        gamma : float\n",
    "            前回のmとvを記憶する割合。\n",
    "        epsilon : float\n",
    "            ゼロ除算を防止するための小さな値。\n",
    "        '''\n",
    "        self.eta=eta\n",
    "        self.gamma=gamma\n",
    "        self.one_minus_gamma=1-gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.name='SDProp η={} γ={} ε={}'.format(eta,gamma,epsilon)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        m=np.repeat(.0,2)\n",
    "        v=np.repeat(.0,2)\n",
    "        for i in range(step):\n",
    "            g=f.grad(w)\n",
    "            m= self.gamma*m + self.one_minus_gamma*g\n",
    "            v= self.gamma*v + self.one_minus_gamma*(g-m)**2\n",
    "            estimated_v=v/(1-self.gamma**(i+1))\n",
    "            w-=self.eta/(np.sqrt(estimated_v)+self.epsilon)*g\n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adastand(DescentMethod2D):\n",
    "    'NTTの考案したもの。'\n",
    "    def __init__(self,eta=0.001,beta1=0.7,beta2=0.99,epsilon=1e-8):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            学習率。\n",
    "        beta1 : float\n",
    "            前回のmを引き継ぐ割合。\n",
    "        beta2 : float\n",
    "            前回のvを引き継ぐ割合。\n",
    "        epsilon : float\n",
    "            ゼロ除算を防止するための小さな値。\n",
    "        '''\n",
    "        self.eta=eta\n",
    "        self.beta1=beta1\n",
    "        self.one_minus_beta1=1-beta1\n",
    "        self.beta2=beta2\n",
    "        self.one_minus_beta2=1-beta2\n",
    "        self.epsilon=epsilon\n",
    "        self.name='Adastand η={} β1={} β2={} ε={}'.format(eta,beta1,beta2,epsilon)\n",
    "    def execute(self,f,w:np.ndarray,step:int):\n",
    "        x_log=[w[0]]\n",
    "        y_log=[w[1]]\n",
    "        m=np.repeat(.0,2)\n",
    "        v=np.repeat(.0,2)\n",
    "        for i in range(step):\n",
    "            g=f.grad(w)\n",
    "            m= self.beta1*m + self.one_minus_beta1*(g-m)\n",
    "            v= self.beta2*v + self.one_minus_beta2*(g-m)**2\n",
    "            estimated_m=m/(1-self.beta1**(i+1))\n",
    "            estimated_v=v/(1-self.beta2**(i+1))\n",
    "            w=w-self.eta/(np.sqrt(estimated_v)+self.epsilon)*estimated_m\n",
    "            x_log.append(w[0])\n",
    "            y_log.append(w[1])\n",
    "        return w,x_log,y_log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
